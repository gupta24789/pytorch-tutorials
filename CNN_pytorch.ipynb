{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype = torch.float)\n",
    "input_2d = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]], dtype = torch.float)\n",
    "input_2d_img = torch.tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], \n",
    "                             [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], \n",
    "                             [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "                              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]], dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]), torch.Size([2, 5]), torch.Size([3, 3, 10]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d.shape, input_2d.shape, input_2d_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are 3 types of convolution operations.\n",
    "\n",
    "   **1D convolution**\n",
    "        - majorly used where the input is sequential such as text or audio.\n",
    "        - Input shape : [batch_size, input_channel, signal_length]\n",
    "            \n",
    "   **2D convolution** \n",
    "       — majorly used where the input is an image.\n",
    "       - Input shape : [batch_size, input_channels, input_height, input_width]\n",
    "\n",
    "   **3D convolution** \n",
    "       — majorly used in 3D medical imaging or detecting events in videos.\n",
    "       \n",
    "       \n",
    "#### Output Image Dimension:\n",
    "    - lower bound of ((n+2p-f)/s) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), torch.Size([1, 1, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1d.unsqueeze(0).shape, input_1d.unsqueeze(0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## shape the shape of input data\n",
    "input_1d = input_1d.unsqueeze(0).unsqueeze(0)\n",
    "input_1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_1: \n",
      "\n",
      "torch.Size([1, 1, 8]) \n",
      "\n",
      "tensor([[[1.2412, 1.5913, 1.9413, 2.2914, 2.6415, 2.9915, 3.3416, 3.6917]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "print(\"cnn1d_1: \\n\")\n",
    "print(cnn1d_1(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_1(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_2: \n",
      "\n",
      "torch.Size([1, 1, 4]) \n",
      "\n",
      "tensor([[[ 0.5194,  0.0885, -0.3425, -0.7734]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2)\n",
    "print(\"cnn1d_2: \\n\")\n",
    "print(cnn1d_2(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_2(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_3: \n",
      "\n",
      "torch.Size([1, 1, 9]) \n",
      "\n",
      "tensor([[[-0.4813, -0.6230, -0.7647, -0.9063, -1.0480, -1.1897, -1.3313,\n",
      "          -1.4730, -1.6147]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1)\n",
    "print(\"cnn1d_3: \\n\")\n",
    "print(cnn1d_3(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_3(input_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_4: \n",
      "\n",
      "torch.Size([1, 5, 8]) \n",
      "\n",
      "tensor([[[-0.2785, -0.1557, -0.0328,  0.0901,  0.2129,  0.3358,  0.4587,\n",
      "           0.5815],\n",
      "         [ 0.1779,  0.2672,  0.3565,  0.4458,  0.5351,  0.6244,  0.7137,\n",
      "           0.8031],\n",
      "         [ 2.0772,  3.1526,  4.2280,  5.3034,  6.3787,  7.4541,  8.5295,\n",
      "           9.6049],\n",
      "         [ 1.4643,  1.8987,  2.3331,  2.7675,  3.2019,  3.6363,  4.0707,\n",
      "           4.5051],\n",
      "         [ 0.9221,  0.8442,  0.7663,  0.6884,  0.6105,  0.5325,  0.4546,\n",
      "           0.3767]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_4 = nn.Conv1d(in_channels=1, out_channels=5, kernel_size=3, stride=1)\n",
    "print(\"cnn1d_4: \\n\")\n",
    "print(cnn1d_4(input_1d).shape, \"\\n\")\n",
    "print(cnn1d_4(input_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1d — Input 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d = input_2d.unsqueeze(0)\n",
    "input_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_5: \n",
      "\n",
      "torch.Size([1, 1, 3]) \n",
      "\n",
      "tensor([[[3.5274, 4.3024, 5.0774]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_5 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=1)\n",
    "print(\"cnn1d_5: \\n\")\n",
    "print(cnn1d_5(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_5(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_6: \n",
      "\n",
      "torch.Size([1, 1, 2]) \n",
      "\n",
      "tensor([[[2.1102, 2.6286]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_6 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, stride=2)\n",
    "print(\"cnn1d_6: \\n\")\n",
    "print(cnn1d_6(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_6(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_7: \n",
      "\n",
      "torch.Size([1, 1, 4]) \n",
      "\n",
      "tensor([[[-1.0319, -1.1590, -1.2860, -1.4131]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_7 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=2, stride=1)\n",
    "print(\"cnn1d_7: \\n\")\n",
    "print(cnn1d_7(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_7(input_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1d_8: \n",
      "\n",
      "torch.Size([1, 5, 3]) \n",
      "\n",
      "tensor([[[-1.4778, -1.6075, -1.7373],\n",
      "         [ 2.5576,  3.2652,  3.9729],\n",
      "         [-3.3112, -4.1502, -4.9892],\n",
      "         [ 4.3007,  5.2509,  6.2010],\n",
      "         [ 1.0870,  1.7911,  2.4953]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "cnn1d_8 = nn.Conv1d(in_channels=2, out_channels=5, kernel_size=3, stride=1)\n",
    "print(\"cnn1d_8: \\n\")\n",
    "print(cnn1d_8(input_2d).shape, \"\\n\")\n",
    "print(cnn1d_8(input_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
       "\n",
       "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n",
       "\n",
       "        [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
       "         [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d_img = input_2d_img.unsqueeze(0)\n",
    "input_2d_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_1: \n",
      "\n",
      "torch.Size([1, 1, 1, 8]) \n",
      "\n",
      "tensor([[[[0.2948, 0.9233, 1.5518, 2.1804, 2.8089, 3.4374, 4.0660, 4.6945]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_1 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=1,)\n",
    "print(\"cnn2d_1: \\n\")\n",
    "print(cnn2d_1(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_1(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_2: \n",
      "\n",
      "torch.Size([1, 1, 1, 4]) \n",
      "\n",
      "tensor([[[[ 0.0374, -0.9646, -1.9667, -2.9688]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3, stride=2)\n",
    "print(\"cnn2d_2: \\n\")\n",
    "print(cnn2d_2(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_2(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_3: \n",
      "\n",
      "torch.Size([1, 1, 2, 9]) \n",
      "\n",
      "tensor([[[[1.2197, 1.9962, 2.7728, 3.5494, 4.3259, 5.1025, 5.8791, 6.6556,\n",
      "           7.4322],\n",
      "          [1.2197, 1.9962, 2.7728, 3.5494, 4.3259, 5.1025, 5.8791, 6.6556,\n",
      "           7.4322]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_3 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=2, stride=1)\n",
    "print(\"cnn2d_3: \\n\")\n",
    "print(cnn2d_3(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_3(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn2d_4: \n",
      "\n",
      "torch.Size([1, 5, 1, 8]) \n",
      "\n",
      "tensor([[[[ 1.3008,  2.2184,  3.1361,  4.0537,  4.9714,  5.8890,  6.8067,\n",
      "            7.7243]],\n",
      "\n",
      "         [[-0.0567, -0.0653, -0.0740, -0.0827, -0.0913, -0.1000, -0.1086,\n",
      "           -0.1173]],\n",
      "\n",
      "         [[-0.0568, -0.3175, -0.5783, -0.8391, -1.0999, -1.3607, -1.6215,\n",
      "           -1.8823]],\n",
      "\n",
      "         [[-0.2346, -0.2252, -0.2157, -0.2062, -0.1968, -0.1873, -0.1778,\n",
      "           -0.1684]],\n",
      "\n",
      "         [[ 1.0794,  1.4350,  1.7906,  2.1463,  2.5019,  2.8575,  3.2131,\n",
      "            3.5687]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "cnn2d_4 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, stride=1)\n",
    "print(\"cnn2d_4: \\n\")\n",
    "print(cnn2d_4(input_2d_img).shape, \"\\n\")\n",
    "print(cnn2d_4(input_2d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
