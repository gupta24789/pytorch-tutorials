# -*- coding: utf-8 -*-
"""9. transfer_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xL72jqmSUeFrX1KEilpCStvScoO7Zwpa
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import datasets, models, transforms

import matplotlib.pyplot as plt

"""## Read Data"""

transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(0.5,0.5,0.5)
])

train_dataset = datasets.CIFAR10(root="./data/", train=True, download=True, transform = transform)
val_dataset = datasets.CIFAR10(root="./data/", train=False, transform = transform)

batch_size = 100
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size= batch_size, shuffle=False)

dataloaders= {
    "train": train_dataloader,
    "val": val_dataloader
}

class_names = {i:val for i,val in enumerate(train_dataset.classes)}
class_names

device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)

def train_model(model, criteria, optimizer, dataloaders,  epochs, scheduler):    
    
    best_model_wts = model.state_dict()
    best_val_acc = 0.0
    
    for epoch in range(epochs):
        
        for phase in ["train","val"]:
            print(f"\n{phase} phase : Epoch : {epoch+1}/{epochs}")
            n_correct = 0
            n_samples = 0
            
            if phase == "train":
              model.train()
            else:
              model.eval()
            
            running_loss = 0.0
            running_accuracy = 0.0
            
            ## Ierate over data
            for i, (images, labels) in enumerate(dataloaders[phase]):
                
                images = images.to(device)
                labels = labels.to(device)
                
                ##forward
                y_pred = model(images)
                loss = criteria(y_pred, labels)
                
                ## weight updates
                if phase=="train":
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    scheduler.step()
                    
                _, predictions = torch.max(y_pred, axis=1)
                n_correct += (predictions==labels).sum()
                n_samples += len(labels)
                accuracy = n_correct/n_samples
                
                if (i+1)%100==0:
                    if phase=="train":
                        print(f"Step : {i+1}/{len(dataloaders[phase])}, Train Loss : {loss.item():.4f} Train accuracy : {accuracy:.4f}")
                    else:
                        print(f"Step : {i+1}/{len(dataloaders[phase])}, Val Loss : {loss.item():.4f} Val accuracy : {accuracy:.4f}")
                        if accuracy>best_val_acc:
                            best_val_acc = accuracy
                            print(f"Best validation Accuarcy : {best_val_acc}")
#                             torch.save(model.state_dict())

"""### Model Fine Tuning"""

model = models.resnet50(pretrained=True)
in_features = model.fc.in_features
model.fc = nn.Linear(in_features, len(class_names))
model = model.to(device)

learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
step_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
criteria = nn.CrossEntropyLoss()
epochs = 3


train_model(model,criteria, optimizer, dataloaders, epochs, step_lr_scheduler)

"""## With Fixed parameters"""

model = models.resnet50(pretrained=True)

for param in model.parameters():
    param.requires_grad = False

in_features = model.fc.in_features    
model.fc = nn.Linear(in_features, len(class_names))    
model = model.to(device)


learning_rate = 0.01
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
step_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
criteria = nn.CrossEntropyLoss()
epochs = 3

train_model(model,criteria, optimizer, dataloaders, epochs, step_lr_scheduler)

