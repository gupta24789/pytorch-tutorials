{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\n",
    "\n",
    "    map-style datasets\n",
    "    iterable-style datasets.\n",
    "\n",
    "\n",
    "#### Map-style datasets\n",
    "\n",
    "    A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n",
    "    For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.\n",
    "\n",
    "#### Iterable-style datasets\n",
    "\n",
    "    An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When input is array or list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "features : array => every item has same length\n",
    "label : array \n",
    "\"\"\"\n",
    "features = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9],\n",
    "    [10,11,12],\n",
    "    [13,14,15],\n",
    "    [16,17,18],\n",
    "    [23,17,18]\n",
    "])\n",
    "\n",
    "labels = [1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : \n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]\n",
      " [13 14 15]\n",
      " [16 17 18]\n",
      " [23 17 18]]\n",
      "Labels : \n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 1\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return (feature, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "print(f\"Features : \\n{features}\")\n",
    "print(f\"Labels : \\n{labels}\")\n",
    "print(\"\")\n",
    "ds = CustomDataset(features, labels)\n",
    "dl = DataLoader(ds, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : \n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.],\n",
      "        [13., 14., 15.],\n",
      "        [16., 17., 18.],\n",
      "        [23., 17., 18.]])\n",
      "Labels : \n",
      "tensor([1, 2, 3, 4, 5, 6, 7])\n",
      "\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 2\n",
    "\"\"\"\n",
    "features : tensor\n",
    "labels : tensor\n",
    "description : Using TensorDataset converting data into datset obj, then using DataLoader class.\n",
    "\"\"\"\n",
    "features = torch.Tensor(features)\n",
    "labels = torch.tensor(labels)\n",
    "print(f\"Features : \\n{features}\")\n",
    "print(f\"Labels : \\n{labels}\")\n",
    "print(\"\")\n",
    "ds = TensorDataset(features, labels)\n",
    "dl = DataLoader(ds, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : \n",
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]\n",
      " [ 7.  8.  9.]\n",
      " [10. 11. 12.]\n",
      " [13. 14. 15.]\n",
      " [16. 17. 18.]\n",
      " [23. 17. 18.]]\n",
      "Labels : \n",
      "[1 2 3 4 5 6 7]\n",
      "\n",
      "Input_data for loader : \n",
      "[(array([1., 2., 3.], dtype=float32), 1), (array([4., 5., 6.], dtype=float32), 2), (array([7., 8., 9.], dtype=float32), 3), (array([10., 11., 12.], dtype=float32), 4), (array([13., 14., 15.], dtype=float32), 5), (array([16., 17., 18.], dtype=float32), 6), (array([23., 17., 18.], dtype=float32), 7)]\n",
      "\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 3\n",
    "\"\"\"\n",
    "Without making dataset object, direct passing the features, labels to DataLoader\n",
    "\"\"\"\n",
    "\n",
    "def data_collator(batch):\n",
    "    print(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "features = features.numpy() if torch.is_tensor(features) else features\n",
    "labels = labels.numpy() if torch.is_tensor(labels) else labels\n",
    "print(f\"Features : \\n{features}\")\n",
    "print(f\"Labels : \\n{labels}\")\n",
    "\n",
    "print(\"\")\n",
    "data_list = list(zip(features, label))\n",
    "print(f\"Input_data for loader : \\n{data_list}\")\n",
    "print(\"\")\n",
    "\n",
    "dl = DataLoader(data_list, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When input is dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"features\": np.array([\n",
    "                [1,2,3],\n",
    "                [4,5,6],\n",
    "                [7,8,9],\n",
    "                [10,11,12],\n",
    "                [13,14,15],\n",
    "                [16,17,18],\n",
    "                [23,17,18]\n",
    "            ]),\n",
    "    \"labels\" : [1,2,3,4,5,6,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      "{'features': array([[ 1,  2,  3],\n",
      "       [ 4,  5,  6],\n",
      "       [ 7,  8,  9],\n",
      "       [10, 11, 12],\n",
      "       [13, 14, 15],\n",
      "       [16, 17, 18],\n",
      "       [23, 17, 18]]), 'labels': [1, 2, 3, 4, 5, 6, 7]}\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 1\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inputs):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.inputs['features'][index]\n",
    "        label = self.inputs['labels'][index]\n",
    "        return (feature, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['features'])\n",
    "\n",
    "\n",
    "print(f\"Inputs : \\n{inputs}\")\n",
    "print(\"\")\n",
    "\n",
    "ds = CustomDataset(inputs)\n",
    "dl = DataLoader(ds, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      "{'features': array([[ 1,  2,  3],\n",
      "       [ 4,  5,  6],\n",
      "       [ 7,  8,  9],\n",
      "       [10, 11, 12],\n",
      "       [13, 14, 15],\n",
      "       [16, 17, 18],\n",
      "       [23, 17, 18]]), 'labels': [1, 2, 3, 4, 5, 6, 7]}\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "## Method - 2\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return (feature, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "print(f\"Inputs : \\n{inputs}\")\n",
    "print(\"\")\n",
    "\n",
    "ds = CustomDataset(inputs['features'], inputs['labels'])\n",
    "dl = DataLoader(ds, batch_size = 3)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      "{'features': array([[ 1,  2,  3],\n",
      "       [ 4,  5,  6],\n",
      "       [ 7,  8,  9],\n",
      "       [10, 11, 12],\n",
      "       [13, 14, 15],\n",
      "       [16, 17, 18],\n",
      "       [23, 17, 18]]), 'labels': [1, 2, 3, 4, 5, 6, 7]}\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 3\n",
    "print(f\"Inputs : \\n{inputs}\")\n",
    "print(\"\")\n",
    "\n",
    "ds = TensorDataset(torch.tensor(inputs['features']), torch.tensor(inputs['labels']))\n",
    "dl = DataLoader(ds, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      "{'features': array([[ 1,  2,  3],\n",
      "       [ 4,  5,  6],\n",
      "       [ 7,  8,  9],\n",
      "       [10, 11, 12],\n",
      "       [13, 14, 15],\n",
      "       [16, 17, 18],\n",
      "       [23, 17, 18]]), 'labels': [1, 2, 3, 4, 5, 6, 7]}\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "## Method - 4\n",
    "print(f\"Inputs : \\n{inputs}\")\n",
    "print(\"\")\n",
    "\n",
    "data = list(zip(inputs['features'], inputs['labels']))\n",
    "dl = DataLoader(data, batch_size = 2)\n",
    "exmaple = next(iter(dl))\n",
    "example_feature, example_label = exmaple\n",
    "print(example_feature)\n",
    "print(example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator\n",
    "\n",
    "    In all the above examples length of every feature is same. But what if every item has different length then above method will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2],'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2],'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2],'label':1},\n",
    "    {'tokenized_input': [1, 17, 2],'label':0},\n",
    "     {'tokenized_input': [1, 23, 2],'label':0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below code will raise an error\n",
    "# loader = DataLoader(nlp_data, batch_size=2, shuffle=False)\n",
    "# batch = next(iter(loader))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do? There are two solutions:\n",
    "\n",
    "Pad the whole dataset to the longest example.\n",
    "Pad dynamically during batch creation.\n",
    "The first solution might seem more straightforward — just expand all examples to the longest one. But there is an issue — we will waste memory and computing power (they are expensive on GPU!) for processing padding, which does not influence the result. It is especially painful if we have a few long sequences in the data, and most of them are relatively short. In such a case, we are mostly process padding instead of data!\n",
    "\n",
    "If we pad the whole dataset to the longest sequence, there is a lot of wasted space! An alternative is to pad the data on the fly. When samples for the batch are selected, we pad only them to the longest one. If we additionally order the data by length, the padding will be minimal. If there are a few very long sequences, they will only influence their batches- not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tokenized_input': [1, 4, 5, 9, 3, 2], 'label': 0}, {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2], 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    print(batch)\n",
    "\n",
    "loader = DataLoader(nlp_data, batch_size=2, collate_fn=custom_collate) \n",
    "next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 0, 'tokenized_input': [1, 4, 5, 9, 3, 2]},\n",
      " {'label': 0, 'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2]},\n",
      " {'label': 1, 'tokenized_input': [1, 30, 67, 117, 21, 15, 2]},\n",
      " {'label': 0, 'tokenized_input': [1, 17, 2]},\n",
      " {'label': 0, 'tokenized_input': [1, 23, 2]}]\n",
      "\n",
      "\n",
      "\n",
      "Tokenized input : [tensor([1, 4, 5, 9, 3, 2]), tensor([  1,   7,   3,  14,  48,   7,  23, 154,   2])]\n",
      "Label :  [0, 0]\n",
      "\n",
      "(tensor([[  1,   1],\n",
      "        [  4,   7],\n",
      "        [  5,   3],\n",
      "        [  9,  14],\n",
      "        [  3,  48],\n",
      "        [  2,   7],\n",
      "        [  0,  23],\n",
      "        [  0, 154],\n",
      "        [  0,   2]]), [0, 0])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate(batch):\n",
    "    tokenized_input = [torch.tensor(item['tokenized_input'] )for item in batch]\n",
    "    label = [item['label'] for item in batch]\n",
    "\n",
    "    print(f'Tokenized input : {tokenized_input}')\n",
    "    print(f'Label :  {label}\\n')\n",
    "\n",
    "    tokenized_input = pad_sequence(tokenized_input)\n",
    "    return (tokenized_input, label)\n",
    "\n",
    "pprint(nlp_data)\n",
    "print(\"\")\n",
    "loader = DataLoader(nlp_data, batch_size=2, collate_fn=custom_collate) \n",
    "print(\"\\n\")\n",
    "print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step:\n",
    "\n",
    "For padding we use pad_sequence.\n",
    "\n",
    "Collate function takes a single argument — a list of examples. In this case, it will be a list of dicts, but it also can be a list of tuples, etc. — depending on the dataset.\n",
    "\n",
    "As data comes if format “list of dicts” we need to traverse it and create a separate list for all inputs and labels. In the meantime, tokenized_input is converted to a 1-D tensor (it was a list of ints).\n",
    "\n",
    "Perform the padding.\n",
    "\n",
    "As labels were a list of ints, we converted it into a tensor.\n",
    "\n",
    "Return formatted batch.\n",
    "\n",
    "Set our custom function in the loader.\n",
    "\n",
    "As we can see, the batch is in the same format as for default collation with a dictionary. We clearly see that amount of padding is only minimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
