{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    One of the first pieces of information in every deep learning course is that we perform training/inferencing in batches. \n",
    "    Most of the time, a batch is just a number of stacked examples. But in some cases, we would like to modify how it is created.\n",
    "\n",
    "    First things first, let’s investigate what happens in the default case. Assume we have the following toy dataset. \n",
    "    It contains four examples, three features each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1  7.4  0. ]\n",
      " [-0.2  5.3  0. ]\n",
      " [ 0.2  8.2  1. ]\n",
      " [ 0.2  7.7  1. ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    [0.1, 7.4, 0],\n",
    "    [-0.2, 5.3, 0],\n",
    "    [0.2, 8.2, 1],\n",
    "    [0.2, 7.7, 1]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1000,  7.4000,  0.0000],\n",
      "        [-0.2000,  5.3000,  0.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise, but let’s formalize what was has been done:\n",
    "\n",
    "- Loader selected 2items from the dataset.\n",
    "- Those items were converted into a tensor (2 items of size 3).\n",
    "- A new tensor was created (2x3) and returned.\n",
    "\n",
    "Default setup also allows us to use dictionaries. Let’s see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
      " {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
      " {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
      " {'x1': 0.2, 'x2': 7.7, 'y': 10}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# now dataset is a list of dicts\n",
    "dict_data = [\n",
    "    {'x1': 0.1, 'x2': 7.4, 'y': 0},\n",
    "    {'x1': -0.2, 'x2': 5.3, 'y': 0},\n",
    "    {'x1': 0.2, 'x2': 8.2, 'y': 1},\n",
    "    {'x1': 0.2, 'x2': 7.7, 'y': 10},\n",
    "]\n",
    "pprint(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x1': tensor([ 0.1000, -0.2000], dtype=torch.float64),\n",
      " 'x2': tensor([7.4000, 5.3000], dtype=torch.float64),\n",
      " 'y': tensor([0, 0])}\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dict_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom collate function\n",
    "\n",
    "If default collation is so smart, why might we need to create a custom one? Default one has a significant limitation \n",
    "\n",
    "— batch data must be in the same dimension. Imagine we have an NLP task, and the data is tokenized text. It will not work and raise an Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m nlp_data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_input\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m      4\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_input\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m154\u001b[39m, \u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m      5\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_input\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m67\u001b[39m, \u001b[38;5;241m117\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m      6\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenized_input\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(nlp_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:160\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: default_collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\saurabh\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "# values are token indices but it does not matter - it can be any kind of variable-size data\n",
    "nlp_data = [\n",
    "    {'tokenized_input': [1, 4, 5, 9, 3, 2],'label':0},\n",
    "    {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2],'label':0},\n",
    "    {'tokenized_input': [1, 30, 67, 117, 21, 15, 2],'label':1},\n",
    "    {'tokenized_input': [1, 17, 2],'label':0},\n",
    "]\n",
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we do? There are two solutions:\n",
    "\n",
    "- Pad the whole dataset to the longest example.\n",
    "- Pad dynamically during batch creation.\n",
    "\n",
    "The first solution might seem more straightforward — just expand all examples to the longest one. But there is an issue — we will waste memory and computing power (they are expensive on GPU!) for processing padding, which does not influence the result. It is especially painful if we have a few long sequences in the data, and most of them are relatively short. In such a case, we are mostly process padding instead of data!\n",
    "\n",
    "\n",
    "If we pad the whole dataset to the longest sequence, there is a lot of wasted space!\n",
    "An alternative is to pad the data on the fly. When samples for the batch are selected, we pad only them to the longest one. If we additionally order the data by length, the padding will be minimal. If there are a few very long sequences, they will only influence their batches- not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    inputs = [torch.tensor(d['tokenized_input']) for d in data] #(3)\n",
    "    labels = [d['label'] for d in data]\n",
    "\n",
    "    inputs = pad_sequence(inputs, batch_first=True) #(4)\n",
    "    labels = torch.tensor(labels) #(5)\n",
    "\n",
    "    return { #(6)\n",
    "        'tokenized_input': inputs,\n",
    "        'label': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokenized_input': [1, 4, 5, 9, 3, 2], 'label': 0},\n",
       " {'tokenized_input': [1, 7, 3, 14, 48, 7, 23, 154, 2], 'label': 0},\n",
       " {'tokenized_input': [1, 30, 67, 117, 21, 15, 2], 'label': 1},\n",
       " {'tokenized_input': [1, 17, 2], 'label': 0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([0, 0]),\n",
      " 'tokenized_input': tensor([[  1,   4,   5,   9,   3,   2,   0,   0,   0],\n",
      "        [  1,   7,   3,  14,  48,   7,  23, 154,   2]])}\n",
      "{'label': tensor([1, 0]),\n",
      " 'tokenized_input': tensor([[  1,  30,  67, 117,  21,  15,   2],\n",
      "        [  1,  17,   2,   0,   0,   0,   0]])}\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(nlp_data, batch_size=2, shuffle=False, collate_fn=custom_collate) #(7)\n",
    "iter_loader = iter(loader)\n",
    "batch1 = next(iter_loader)\n",
    "pprint(batch1)\n",
    "batch2 = next(iter_loader)\n",
    "pprint(batch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step:\n",
    "\n",
    "For padding we use pad_sequence.\n",
    "\n",
    "Collate function takes a single argument — a list of examples. In this case, it will be a list of dicts, but it also can be a list of tuples, etc. — depending on the dataset.\n",
    "\n",
    "As data comes if format “list of dicts” we need to traverse it and create a separate list for all inputs and labels. In the meantime, tokenized_input is converted to a 1-D tensor (it was a list of ints).\n",
    "\n",
    "Perform the padding.\n",
    "\n",
    "As labels were a list of ints, we converted it into a tensor.\n",
    "\n",
    "Return formatted batch.\n",
    "\n",
    "Set our custom function in the loader.\n",
    "\n",
    "As we can see, the batch is in the same format as for default collation with a dictionary. We clearly see that amount of padding is only minimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
